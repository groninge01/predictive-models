{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation_functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model, tree, discriminant_analysis, naive_bayes, ensemble, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPL Machine Learning Walkthrough\n",
    "\n",
    "## 03. Model Building & Hyperparameter Tuning\n",
    "Welcome to the third part of this Machine Learning Walkthrough. This tutorial will focus on the model building process, including how to tune hyperparameters. In the [next tutorial], we will create weekly predictions based on the model we have created here.\n",
    "\n",
    "Specifically, this tutorial will cover a few things:\n",
    "\n",
    "1. Choosing which Machine Learning algorithm to use from a variety of choices\n",
    "2. Hyperparameter Tuning\n",
    "3. Overfitting/Underfitting\n",
    "\n",
    "### Choosing an Algorithm\n",
    "The best way to decide on specific algorithm to use, is to try them all! To do this, we will define a function which we first used in our AFL Predictions tutorial. This will iterate over a number of algorithms and give us a good indication of which algorithms are suited for this dataset and exercise.\n",
    "\n",
    "Let's first use grab the features we created in the last tutorial. This may take a minute or two to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating all games feature DataFrame\n",
      "Creating stats feature DataFrame\n",
      "Creating odds feature DataFrame\n",
      "Creating market values feature DataFrame\n",
      "Filling NAs\n",
      "Merging stats, odds and market values into one features DataFrame\n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "features = create_feature_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start our modelling process, we need to make a training set, a test set and a holdout set. As we are using cross validation, we will make our training set all of the seasons up until 2018/19, and we will use the 2018/19 season as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [col for col in features.columns if col.startswith(\"f_\")]\n",
    "betting_features = []\n",
    "\n",
    "le = LabelEncoder() # Initiate a label encoder to transform the labels 'away', 'draw', 'home' to 0, 1, 2\n",
    "\n",
    "# Grab all seasons except for 18/19 to use CV with\n",
    "all_x = features.loc[features.season != '1819', ['gameId'] + feature_list]\n",
    "all_y = features.loc[features.season != '1819', 'result']\n",
    "all_y = le.fit_transform(all_y)\n",
    "\n",
    "# Create our training vector as the seasons except 17/18 and 18/19\n",
    "train_x = features.loc[~features.season.isin(['1718', '1819']), ['gameId'] + feature_list]\n",
    "train_y = le.transform(features.loc[~features.season.isin(['1718', '1819']), 'result'])\n",
    "\n",
    "# Create our holdout vectors as the 17/18 season\n",
    "holdout_x = features.loc[features.season == '1718', ['gameId'] + feature_list]\n",
    "holdout_y = le.transform(features.loc[features.season == '1718', 'result'])\n",
    "\n",
    "# Create our test vectors as the 18/19 season\n",
    "test_x = features.loc[features.season == '1819', ['gameId'] + feature_list]\n",
    "test_y = le.transform(features.loc[features.season == '1819', 'result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_152-release\"; OpenJDK Runtime Environment (build 1.8.0_152-release-1056-b12); OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode)\n",
      "  Starting server from /home/groninge/anaconda3/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpa5g2821u\n",
      "  JVM stdout: /tmp/tmpa5g2821u/h2o_groninge_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpa5g2821u/h2o_groninge_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Amsterdam</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_groninge_u1gi8k</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.111 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Europe/Amsterdam\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.5\n",
       "H2O cluster version age:    4 days\n",
       "H2O cluster name:           H2O_from_python_groninge_u1gi8k\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.111 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.7.3 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://localhost:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Amsterdam</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.5</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_groninge_u1gi8k</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.111 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Europe/Amsterdam\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.5\n",
       "H2O cluster version age:    4 days\n",
       "H2O cluster name:           H2O_from_python_groninge_u1gi8k\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.111 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.7.3 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<H2OConnection to http://localhost:54321, no session>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h2o\n",
    "\n",
    "h2o.init(nthreads = -1, max_mem_size = 8)\n",
    "h2o.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train = features.loc[~features.season.isin(['1718', '1819']), ['result'] + feature_list]\n",
    "valid = features.loc[features.season == '1718', ['result'] + feature_list]\n",
    "test = features.loc[features.season == '1819', ['result'] + feature_list]\n",
    "\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "valid.to_csv(\"data/valid.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "train_h2o = h2o.import_file(\"data/train.csv\")\n",
    "valid_h2o = h2o.import_file(\"data/valid.csv\")\n",
    "test_h2o = h2o.import_file(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(\n",
    "    max_runtime_secs = 60 * 30,\n",
    "    max_models = 100,\n",
    "    stopping_metric = \"logloss\",\n",
    "    sort_metric = \"logloss\",\n",
    "    balance_classes = True,\n",
    "    seed = 183)\n",
    "\n",
    "aml.train(y='result', training_frame=train_h2o, validation_frame=valid_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GLM_grid_1_AutoML_20190920_224105_model_1          </td><td style=\"text-align: right;\">              0.550189</td><td style=\"text-align: right;\"> 0.96451 </td><td style=\"text-align: right;\">0.605731</td><td style=\"text-align: right;\">0.36691 </td></tr>\n",
       "<tr><td>GLM_grid_1_AutoML_20190920_223735_model_1          </td><td style=\"text-align: right;\">              0.550189</td><td style=\"text-align: right;\"> 0.96451 </td><td style=\"text-align: right;\">0.605731</td><td style=\"text-align: right;\">0.36691 </td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20190920_224105</td><td style=\"text-align: right;\">              0.545782</td><td style=\"text-align: right;\"> 0.965398</td><td style=\"text-align: right;\">0.605491</td><td style=\"text-align: right;\">0.366619</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_AutoML_20190920_223735   </td><td style=\"text-align: right;\">              0.549286</td><td style=\"text-align: right;\"> 0.965968</td><td style=\"text-align: right;\">0.605848</td><td style=\"text-align: right;\">0.367052</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20190920_223735</td><td style=\"text-align: right;\">              0.54923 </td><td style=\"text-align: right;\"> 0.966075</td><td style=\"text-align: right;\">0.605942</td><td style=\"text-align: right;\">0.367166</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190920_224105_model_16         </td><td style=\"text-align: right;\">              0.548762</td><td style=\"text-align: right;\"> 0.96982 </td><td style=\"text-align: right;\">0.607167</td><td style=\"text-align: right;\">0.368652</td></tr>\n",
       "<tr><td>XGBoost_3_AutoML_20190920_224105                   </td><td style=\"text-align: right;\">              0.548354</td><td style=\"text-align: right;\"> 0.972219</td><td style=\"text-align: right;\">0.607742</td><td style=\"text-align: right;\">0.369351</td></tr>\n",
       "<tr><td>XGBoost_3_AutoML_20190920_223735                   </td><td style=\"text-align: right;\">              0.548354</td><td style=\"text-align: right;\"> 0.972219</td><td style=\"text-align: right;\">0.607742</td><td style=\"text-align: right;\">0.369351</td></tr>\n",
       "<tr><td>XGBoost_grid_1_AutoML_20190920_224105_model_4      </td><td style=\"text-align: right;\">              0.551821</td><td style=\"text-align: right;\"> 0.973544</td><td style=\"text-align: right;\">0.608221</td><td style=\"text-align: right;\">0.369932</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190920_224105_model_21         </td><td style=\"text-align: right;\">              0.551776</td><td style=\"text-align: right;\"> 0.974303</td><td style=\"text-align: right;\">0.609266</td><td style=\"text-align: right;\">0.371205</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-3 Hit Ratios: "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>hit_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.545213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.789894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  hit_ratio\n",
       "0  1   0.545213\n",
       "1  2   0.789894\n",
       "2  3   1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>away</th>\n",
       "      <th>draw</th>\n",
       "      <th>home</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>51 / 106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99 / 99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>21 / 171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.454787</td>\n",
       "      <td>171 / 376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   away  draw   home     Error       Rate\n",
       "0  55.0   0.0   51.0  0.481132   51 / 106\n",
       "1  22.0   0.0   77.0  1.000000    99 / 99\n",
       "2  21.0   0.0  150.0  0.122807   21 / 171\n",
       "3  98.0   0.0  278.0  0.454787  171 / 376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.confusion_matrix(valid_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/groninge/projects/predictive-models/epl/data/mymodel/GLM_grid_1_AutoML_20190920_224105_model_1\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model_path = h2o.save_model(model=aml.leader, path=\"data/mymodel\", force=True)\n",
    "\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of standard classifiers\n",
    "classifiers = [\n",
    "\n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    xgb.XGBClassifier()    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_algorithms(classifier_list, X, y):\n",
    "    # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
    "    # Cross validate model with Kfold stratified cross validation\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Grab the cross validation scores for each algorithm\n",
    "    cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\", cv = kfold) for classifier in classifier_list]\n",
    "    cv_means = [cv_result.mean() * -1 for cv_result in cv_results]\n",
    "    cv_std = [cv_result.std() for cv_result in cv_results]\n",
    "    algorithm_names = [alg.__class__.__name__ for alg in classifiers]\n",
    "    \n",
    "    # Create a DataFrame of all the CV results\n",
    "    cv_results = pd.DataFrame({\n",
    "        \"Mean Log Loss\": cv_means,\n",
    "        \"Log Loss Std\": cv_std,\n",
    "        \"Algorithm\": algorithm_names\n",
    "    }).sort_values(by='Mean Log Loss')\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_results = find_best_algorithms(classifiers, all_x, all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Log Loss</th>\n",
       "      <th>Log Loss Std</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966852</td>\n",
       "      <td>0.008842</td>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.016643</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>BernoulliNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.046766</td>\n",
       "      <td>0.101457</td>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.093913</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.096977</td>\n",
       "      <td>0.139928</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.105117</td>\n",
       "      <td>0.089326</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.111492</td>\n",
       "      <td>0.201660</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.112677</td>\n",
       "      <td>0.173546</td>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.453923</td>\n",
       "      <td>0.274825</td>\n",
       "      <td>BaggingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.330895</td>\n",
       "      <td>4.185581</td>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.119589</td>\n",
       "      <td>2.432358</td>\n",
       "      <td>GaussianNB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean Log Loss  Log Loss Std                      Algorithm\n",
       "0        0.966852      0.008842           LogisticRegressionCV\n",
       "1        1.016643      0.009693                    BernoulliNB\n",
       "3        1.046766      0.101457     LinearDiscriminantAnalysis\n",
       "5        1.093913      0.008338             AdaBoostClassifier\n",
       "11       1.096977      0.139928                  XGBClassifier\n",
       "10       1.098612      0.000000      GaussianProcessClassifier\n",
       "8        1.105117      0.089326     GradientBoostingClassifier\n",
       "9        2.111492      0.201660         RandomForestClassifier\n",
       "7        2.112677      0.173546           ExtraTreesClassifier\n",
       "6        2.453923      0.274825              BaggingClassifier\n",
       "4        5.330895      4.185581  QuadraticDiscriminantAnalysis\n",
       "2        6.119589      2.432358                     GaussianNB"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that LogisticRegression seems to perform the best out of all the algorithms, and some algorithms have a very high log loss. This is most likely due to overfitting. It would definitely be useful to condense our features down to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "For now, however, we will use logistic regression. Let's first try and tune a logistic regression model with cross validation. To do this, we will use [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization). Grid search essentially tries out each combination of values and finds the model with the lowest error metric, which in our case is log loss. 'C' in logistic regression determines the amount of regularization. Lower values increase regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best log loss: 0.9677573033266084\n",
      "Best lr params: {'C': 0.05, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# Define our parameters to run a grid search over\n",
    "lr_grid = {\n",
    "    \"C\": [0.0001, 0.01, 0.05, 0.2, 1],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(), param_grid=lr_grid, cv=kfold, scoring='neg_log_loss')\n",
    "gs.fit(all_x, all_y)\n",
    "print(\"Best log loss: {}\".format(gs.best_score_ *-1))\n",
    "best_lr_params = gs.best_params_\n",
    "print(\"Best lr params: {}\".format(best_lr_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Baseline\n",
    "We should also define a baseline, as we don't really know if our log loss is good or bad. Randomly assigning a 1/3 chance to each selection yields a log loss of log3 = 1.09. However, what we are really interested in, is how our model performs relative to the odds. So let's find the log loss of the odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9591793301858665"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the log loss of the odds\n",
    "log_loss(all_y, 1 / all_x[['f_awayOdds', 'f_drawOdds', 'f_homeOdds']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good news; our algorithm almost beats the bookies in terms of log loss. It would be great if we could beat this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the Errors Made\n",
    "Now that we have a logistic regression model tuned, let's see what type of errors it made. To do this we will look at the confusion matrix produced when we predict our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(**best_lr_params) # Instantiate the model\n",
    "lr.fit(train_x, train_y) # Fit our model\n",
    "lr_predict = lr.predict(holdout_x) # Predict the holdout values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>away</th>\n",
       "      <th>draw</th>\n",
       "      <th>home</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>draw</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  away  draw  home\n",
       "Actual                     \n",
       "away         58     0    48\n",
       "draw         25     0    74\n",
       "home         22     0   149"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "c_matrix = (pd.DataFrame(confusion_matrix(holdout_y, lr_predict), columns=le.classes_, index=le.classes_)\n",
    " .rename_axis('Actual')\n",
    " .rename_axis('Predicted', axis='columns'))\n",
    "\n",
    "c_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when we predicted 'away' as the result, we correctly predicted 79 / 109 results, a hit rate of 70.6%. However, when we look at our draw hit rate, we only predicted 6 / 84 correctly, meaning we only had a hit rate of around 8.3%. For a more in depth analysis of our predictions, please skip to the Analysing Predictions & Staking Strategies section of the tutorial.\n",
    "\n",
    "Before we move on, however, let's use our model to predict the 17/18 season and compare how we went with the odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity, hit rate, recall, or true positive rate: Actual\n",
      "away    0.547170\n",
      "draw    0.000000\n",
      "home    0.871345\n",
      "dtype: float64\n",
      "\n",
      "Specificity or true negative rate Predicted\n",
      "away    0.825926\n",
      "draw    1.000000\n",
      "home    0.404878\n",
      "dtype: float64\n",
      "\n",
      "Precision or positive predictive value: Predicted\n",
      "away    0.552381\n",
      "draw         NaN\n",
      "home    0.549815\n",
      "dtype: float64\n",
      "\n",
      "Negative predictive value: Predicted\n",
      "away    0.822878\n",
      "draw    0.736702\n",
      "home    0.790476\n",
      "dtype: float64\n",
      "\n",
      "Fall out or false positive rate: Predicted\n",
      "away    0.174074\n",
      "draw    0.000000\n",
      "home    0.595122\n",
      "dtype: float64\n",
      "\n",
      "False negative rate: Actual\n",
      "away    0.452830\n",
      "draw    1.000000\n",
      "home    0.128655\n",
      "dtype: float64\n",
      "\n",
      "False discovery rate: Predicted\n",
      "away    0.447619\n",
      "draw         NaN\n",
      "home    0.450185\n",
      "dtype: float64\n",
      "\n",
      "Overall accuracy: Predicted\n",
      "away    0.747340\n",
      "draw    0.736702\n",
      "home    0.617021\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "FP = c_matrix.sum(axis=0) - np.diag(c_matrix)  \n",
    "FN = c_matrix.sum(axis=1) - np.diag(c_matrix)\n",
    "TP = np.diag(c_matrix)\n",
    "TN = c_matrix.values.sum() - (FP + FN + TP)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print(\"Sensitivity, hit rate, recall, or true positive rate: {}\\n\".format(TPR))\n",
    "print(\"Specificity or true negative rate {}\\n\".format(TNR))\n",
    "print(\"Precision or positive predictive value: {}\\n\".format(PPV))\n",
    "print(\"Negative predictive value: {}\\n\".format(NPV))\n",
    "print(\"Fall out or false positive rate: {}\\n\".format(FPR))\n",
    "print(\"False negative rate: {}\\n\".format(FNR))\n",
    "print(\"False discovery rate: {}\\n\".format(FDR))\n",
    "print(\"Overall accuracy: {}\".format(ACC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our predictions for the 2018/19 season have a log loss of: 0.89451 and an accuracy of: 0.59\n"
     ]
    }
   ],
   "source": [
    "# Get test predictions\n",
    "\n",
    "test_lr = LogisticRegression(**best_lr_params)\n",
    "test_lr.fit(all_x, all_y)\n",
    "test_predictions_probs = lr.predict_proba(test_x)\n",
    "test_predictions = lr.predict(test_x)\n",
    "\n",
    "test_ll = log_loss(test_y, test_predictions_probs)\n",
    "test_accuracy = (test_predictions == test_y).mean()\n",
    "\n",
    "print(\"Our predictions for the 2018/19 season have a log loss of: {0:.5f} and an accuracy of: {1:.2f}\".format(test_ll, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds predictions for the 2018/19 season have a log loss of: 0.89290 and an accuracy of: 0.584\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy and log loss based on the odds\n",
    "odds_ll = log_loss(test_y, 1 / test_x[['f_awayOdds', 'f_drawOdds', 'f_homeOdds']])\n",
    "\n",
    "odds_predictions = test_x[['f_awayOdds', 'f_drawOdds', 'f_homeOdds']].apply(lambda row: row.idxmin()[2:6], axis=1).values\n",
    "odds_accuracy = (odds_predictions == le.inverse_transform(test_y)).mean()\n",
    "\n",
    "print(\"Odds predictions for the 2018/19 season have a log loss of: {0:.5f} and an accuracy of: {1:.3f}\".format(odds_ll, odds_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "There we have it! The odds predicted 60% of EPL games correctly in the 2018/19 season, whilst our model predicted 60% correctly. This is a decent result for the first iteration of our model. In future iterations, we could wait a certain number of matches each season and calculate EMAs for on those first n games. This may help the issue of players switching clubs and teams becoming relatively stronger/weaker compared to previous seasons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
